# local_history_server.py

from flask import Flask, request, jsonify
from queue import Queue, Empty
import logging
import uuid
import threading

# --- é…ç½® ---
log = logging.getLogger('werkzeug')
log.setLevel(logging.ERROR)
app = Flask(__name__)

# --- æ•°æ®å­˜å‚¨ ---
INJECTION_JOBS = Queue()
PROMPT_JOBS = Queue()
TOOL_RESULT_JOBS = Queue()
MODEL_FETCH_JOBS = Queue() # ã€æ–°ã€‘ä¸ºè·å–æ¨¡å‹åˆ—è¡¨åˆ›å»ºçš„é˜Ÿåˆ—
# RESULTS ç°åœ¨ä¸ºæ¯ä¸ªä»»åŠ¡å­˜å‚¨ä¸€ä¸ªåŒ…å«çŠ¶æ€å’Œæµæ•°æ®é˜Ÿåˆ—çš„å­—å…¸
RESULTS = {}
# ã€æ–°ã€‘ç”¨äºç¼“å­˜ä»æ²¹çŒ´è„šæœ¬è·å–çš„æ¨¡å‹æ•°æ®
REPORTED_MODELS_CACHE = {
    "data": None,
    "timestamp": 0,
    "event": threading.Event() # ç”¨äºé€šçŸ¥ç­‰å¾…æ–¹æ•°æ®å·²åˆ°è¾¾
}


# --- API ç«¯ç‚¹ ---

@app.route('/')
def index():
    return "å†å²ç¼–è¾‘ä»£ç†æœåŠ¡å™¨ v6.0 (Model Fetcher Ready) æ­£åœ¨è¿è¡Œã€‚"

# --- æ³¨å…¥ API (æ— å˜åŒ–) ---
@app.route('/submit_injection_job', methods=['POST'])
def submit_injection_job():
    job_data = request.json
    INJECTION_JOBS.put(job_data)
    print(f"âœ… å·²æ¥æ”¶åˆ°æ–°çš„ã€æ³¨å…¥ä»»åŠ¡ã€‘ã€‚æ³¨å…¥é˜Ÿåˆ—ç°æœ‰ä»»åŠ¡: {INJECTION_JOBS.qsize()}ã€‚")
    return jsonify({"status": "success", "message": "Injection job submitted"}), 200

@app.route('/get_injection_job', methods=['GET'])
def get_injection_job():
    try:
        job = INJECTION_JOBS.get_nowait()
        print(f"ğŸš€ History Forger å·²å–èµ°æ³¨å…¥ä»»åŠ¡ã€‚é˜Ÿåˆ—å‰©ä½™: {INJECTION_JOBS.qsize()}ã€‚")
        return jsonify({"status": "success", "job": job}), 200
    except Empty:
        return jsonify({"status": "empty"}), 200

# --- äº¤äº’å¼å¯¹è¯ API (å‡çº§ä»¥æ”¯æŒæµå¼ä¼ è¾“) ---

@app.route('/submit_prompt', methods=['POST'])
def submit_prompt():
    data = request.json
    if not data or 'prompt' not in data:
        return jsonify({"status": "error", "message": "éœ€è¦ 'prompt' å­—æ®µã€‚"}), 400
    
    task_id = str(uuid.uuid4())
    job = {"task_id": task_id, "prompt": data['prompt']}
    PROMPT_JOBS.put(job)
    # ä¸ºæ–°ä»»åŠ¡åˆå§‹åŒ–ç»“æœå­˜å‚¨ï¼ŒåŒ…æ‹¬ä¸€ä¸ªä¸“ç”¨çš„æµé˜Ÿåˆ—
    RESULTS[task_id] = {
        "status": "pending",
        "stream_queue": Queue(),
        "full_response": None
    }
    print(f"âœ… å·²æ¥æ”¶åˆ°æ–°çš„ã€å¯¹è¯ä»»åŠ¡ã€‘(ID: {task_id[:8]})ã€‚å¯¹è¯é˜Ÿåˆ—ç°æœ‰ä»»åŠ¡: {PROMPT_JOBS.qsize()}ã€‚")
    return jsonify({"status": "success", "task_id": task_id}), 200

@app.route('/get_prompt_job', methods=['GET'])
def get_prompt_job():
    try:
        job = PROMPT_JOBS.get_nowait()
        print(f"ğŸš€ Automator å·²å–èµ°å¯¹è¯ä»»åŠ¡ (ID: {job['task_id'][:8]})ã€‚é˜Ÿåˆ—å‰©ä½™: {PROMPT_JOBS.qsize()}ã€‚")
        return jsonify({"status": "success", "job": job}), 200
    except Empty:
        return jsonify({"status": "empty"}), 200

# --- ã€ã€ã€æ–°ã€‘ã€‘ã€‘æµå¼æ•°æ® API ---

@app.route('/stream_chunk', methods=['POST'])
def stream_chunk():
    """æ¥æ”¶æ²¹çŒ´è„šæœ¬å‘é€çš„æµå¼æ•°æ®å—"""
    data = request.json
    task_id = data.get('task_id')
    chunk = data.get('chunk')
    
    # ã€ã€ã€è°ƒè¯•æ—¥å¿—ã€‘ã€‘ã€‘
    print(f"\n--- ğŸ“¥ [Local Server] æ”¶åˆ°æ¥è‡ª Automator çš„æ•°æ®å— (Task ID: {task_id[:8]}) ---")
    print(chunk)
    print("--------------------------------------------------------------------")
    
    if task_id in RESULTS:
        # å°†æ•°æ®å—ï¼ˆæˆ–ç»“æŸä¿¡å·ï¼‰æ”¾å…¥å¯¹åº”ä»»åŠ¡çš„é˜Ÿåˆ—ä¸­
        RESULTS[task_id]['stream_queue'].put(chunk)
        return jsonify({"status": "success"}), 200
    return jsonify({"status": "error", "message": "æ— æ•ˆçš„ä»»åŠ¡ ID"}), 404

@app.route('/get_chunk/<task_id>', methods=['GET'])
def get_chunk(task_id):
    """Python å®¢æˆ·ç«¯ä»æ­¤ç«¯ç‚¹è½®è¯¢æ•°æ®å—"""
    if task_id in RESULTS:
        try:
            # éé˜»å¡åœ°ä»é˜Ÿåˆ—ä¸­è·å–æ•°æ®
            chunk = RESULTS[task_id]['stream_queue'].get_nowait()
            # ã€ã€ã€è°ƒè¯•æ—¥å¿—ã€‘ã€‘ã€‘
            print(f"\n--- ğŸ“¤ [Local Server] API ç½‘å…³å·²å–èµ°æ•°æ®å— (Task ID: {task_id[:8]}) ---")
            print(chunk)
            print("------------------------------------------------------------------")
            return jsonify({"status": "ok", "chunk": chunk}), 200
        except Empty:
            # å¦‚æœé˜Ÿåˆ—ä¸ºç©ºï¼Œæ£€æŸ¥ä»»åŠ¡æ˜¯å¦å·²å®Œæˆ
            if RESULTS[task_id]['status'] in ['completed', 'failed']:
                return jsonify({"status": "done"}), 200
            else:
                return jsonify({"status": "empty"}), 200
    return jsonify({"status": "not_found"}), 404
    
@app.route('/report_result', methods=['POST'])
def report_result():
    """å½“æ²¹çŒ´è„šæœ¬ç¡®è®¤æ•´ä¸ªå¯¹è¯ç»“æŸåï¼Œè°ƒç”¨æ­¤æ¥å£æ¥æœ€ç»ˆç¡®å®šä»»åŠ¡çŠ¶æ€"""
    data = request.json
    task_id = data.get('task_id')
    if task_id and task_id in RESULTS:
        RESULTS[task_id]['status'] = data.get('status', 'completed')
        RESULTS[task_id]['full_response'] = data.get('content', '') # å­˜å‚¨æœ€ç»ˆçš„å®Œæ•´å“åº”ä»¥ä¾›è°ƒè¯•
        print(f"âœ”ï¸ ä»»åŠ¡ {task_id[:8]} å·²å®Œæˆã€‚çŠ¶æ€: {RESULTS[task_id]['status']}ã€‚")
        return jsonify({"status": "success"}), 200
    return jsonify({"status": "error", "message": "æ— æ•ˆçš„ä»»åŠ¡ IDã€‚"}), 404

# --- ã€ã€ã€æ–°ã€‘ã€‘ã€‘å·¥å…·å‡½æ•°ç»“æœ API ---

@app.route('/submit_tool_result', methods=['POST'])
def submit_tool_result():
    """æ¥æ”¶æ¥è‡ª OpenAI ç½‘å…³çš„å·¥å…·å‡½æ•°æ‰§è¡Œç»“æœï¼Œå¹¶ä¸ºå“åº”æµå‡†å¤‡å¥½å­˜å‚¨ç©ºé—´"""
    data = request.json
    if not data or 'task_id' not in data or 'result' not in data:
        return jsonify({"status": "error", "message": "éœ€è¦ 'task_id' å’Œ 'result' å­—æ®µã€‚"}), 400
    
    task_id = data['task_id']
    job = {"task_id": task_id, "result": data['result']}
    TOOL_RESULT_JOBS.put(job)

    # ã€ã€ã€æ ¸å¿ƒä¿®å¤ã€‘ã€‘ã€‘ä¸ºè¿™ä¸ªæ–°ä»»åŠ¡åˆå§‹åŒ–ç»“æœå­˜å‚¨ï¼Œå¦åˆ™åç»­çš„æµæ•°æ®å°†æ— å¤„å®‰æ”¾
    RESULTS[task_id] = {
        "status": "pending",
        "stream_queue": Queue(),
        "full_response": None
    }
    
    print(f"âœ… å·²æ¥æ”¶åˆ°æ–°çš„ã€å·¥å…·è¿”å›ä»»åŠ¡ã€‘(ID: {task_id[:8]}) å¹¶å·²ä¸ºå…¶å‡†å¤‡å¥½æµæ¥æ”¶é˜Ÿåˆ—ã€‚å·¥å…·é˜Ÿåˆ—ç°æœ‰ä»»åŠ¡: {TOOL_RESULT_JOBS.qsize()}ã€‚")
    return jsonify({"status": "success"}), 200

@app.route('/get_tool_result_job', methods=['GET'])
def get_tool_result_job():
    """ä¾› Automator æ²¹çŒ´è„šæœ¬è·å–å·¥å…·å‡½æ•°è¿”å›ä»»åŠ¡"""
    try:
        job = TOOL_RESULT_JOBS.get_nowait()
        print(f"ğŸš€ Automator å·²å–èµ°å·¥å…·è¿”å›ä»»åŠ¡ (ID: {job['task_id'][:8]})ã€‚é˜Ÿåˆ—å‰©ä½™: {TOOL_RESULT_JOBS.qsize()}ã€‚")
        return jsonify({"status": "success", "job": job}), 200
    except Empty:
        return jsonify({"status": "empty"}), 200

# --- ã€ã€ã€æ–°ã€‘ã€‘ã€‘æ¨¡å‹è·å– API ---

@app.route('/submit_model_fetch_job', methods=['POST'])
def submit_model_fetch_job():
    """ç”± OpenAI ç½‘å…³è°ƒç”¨ï¼Œåˆ›å»ºä¸€ä¸ªâ€œè·å–æ¨¡å‹åˆ—è¡¨â€çš„ä»»åŠ¡"""
    if not MODEL_FETCH_JOBS.empty():
        return jsonify({"status": "success", "message": "A fetch job is already pending."}), 200
    
    task_id = str(uuid.uuid4())
    job = {"task_id": task_id, "type": "FETCH_MODELS"}
    MODEL_FETCH_JOBS.put(job)
    
    # é‡ç½®äº‹ä»¶ï¼Œä»¥ä¾¿æ–°çš„è¯·æ±‚å¯ä»¥ç­‰å¾…
    REPORTED_MODELS_CACHE['event'].clear()
    REPORTED_MODELS_CACHE['data'] = None

    print(f"âœ… å·²æ¥æ”¶åˆ°æ–°çš„ã€æ¨¡å‹è·å–ä»»åŠ¡ã€‘(ID: {task_id[:8]})ã€‚")
    return jsonify({"status": "success", "task_id": task_id})

@app.route('/get_model_fetch_job', methods=['GET'])
def get_model_fetch_job():
    """ç”± Model Fetcher æ²¹çŒ´è„šæœ¬è½®è¯¢ï¼Œä»¥æ£€æŸ¥æ˜¯å¦æœ‰å¾…å¤„ç†çš„è·å–ä»»åŠ¡"""
    try:
        job = MODEL_FETCH_JOBS.queue[0] # æŸ¥çœ‹ä»»åŠ¡ä½†ä¸å–å‡º
        return jsonify({"status": "success", "job": job}), 200
    except IndexError:
        return jsonify({"status": "empty"}), 200

@app.route('/acknowledge_model_fetch_job', methods=['POST'])
def acknowledge_model_fetch_job():
    """Model Fetcher åœ¨æ”¶åˆ°ä»»åŠ¡å¹¶å‡†å¤‡åˆ·æ–°é¡µé¢å‰è°ƒç”¨æ­¤æ¥å£ï¼Œä»¥ä»é˜Ÿåˆ—ä¸­å®‰å…¨åœ°ç§»é™¤ä»»åŠ¡"""
    try:
        job = MODEL_FETCH_JOBS.get_nowait()
        print(f"ğŸš€ Model Fetcher å·²ç¡®è®¤å¹¶å–èµ°æ¨¡å‹è·å–ä»»åŠ¡ (ID: {job['task_id'][:8]})ã€‚")
        return jsonify({"status": "success"}), 200
    except Empty:
        return jsonify({"status": "error", "message": "No job to acknowledge."}), 400


@app.route('/report_models', methods=['POST'])
def report_models():
    """ç”± Model Fetcher æ²¹çŒ´è„šæœ¬è°ƒç”¨ï¼Œä»¥å‘é€æ‹¦æˆªåˆ°çš„åŸå§‹æ¨¡å‹æ•°æ®"""
    data = request.json
    models_json = data.get('models_json')
    if models_json:
        REPORTED_MODELS_CACHE['data'] = models_json
        REPORTED_MODELS_CACHE['timestamp'] = uuid.uuid4().int # ä½¿ç”¨UUIDç¡®ä¿æ—¶é—´æˆ³å”¯ä¸€
        REPORTED_MODELS_CACHE['event'].set() # é€šçŸ¥æ‰€æœ‰ç­‰å¾…æ–¹ï¼Œæ•°æ®å·²åˆ°è¾¾
        print(f"âœ”ï¸ æˆåŠŸæ¥æ”¶å¹¶ç¼“å­˜äº†æ–°çš„æ¨¡å‹åˆ—è¡¨æ•°æ®ã€‚")
        return jsonify({"status": "success"}), 200
    return jsonify({"status": "error", "message": "éœ€è¦ 'models_json' å­—æ®µã€‚"}), 400

@app.route('/get_reported_models', methods=['GET'])
def get_reported_models():
    """ç”± OpenAI ç½‘å…³è°ƒç”¨ï¼Œä»¥è·å–ç¼“å­˜çš„æ¨¡å‹æ•°æ®ã€‚å¦‚æœæ•°æ®ä¸å­˜åœ¨ï¼Œå°†ç­‰å¾…ã€‚"""
    # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®ï¼Œæˆ–è€…ç­‰å¾…äº‹ä»¶è¢«è®¾ç½®
    wait_result = REPORTED_MODELS_CACHE['event'].wait(timeout=60) # ç­‰å¾…æœ€å¤š60ç§’
    if not wait_result:
        return jsonify({"status": "error", "message": "ç­‰å¾…æ¨¡å‹æ•°æ®è¶…æ—¶ (60 ç§’)ã€‚"}), 408

    if REPORTED_MODELS_CACHE['data']:
        return jsonify({
            "status": "success",
            "data": REPORTED_MODELS_CACHE['data'],
            "timestamp": REPORTED_MODELS_CACHE['timestamp']
        }), 200
    else:
        # è¿™ç§æƒ…å†µç†è®ºä¸Šä¸åº”è¯¥å‘ç”Ÿï¼Œå› ä¸ºäº‹ä»¶è¢«è®¾ç½®äº†
        return jsonify({"status": "error", "message": "æ•°æ®è·å–å¤±è´¥ï¼Œå³ä½¿äº‹ä»¶å·²è§¦å‘ã€‚"}), 500


if __name__ == '__main__':
    print("======================================================================")
    print("  å†å²ç¼–è¾‘ä»£ç†æœåŠ¡å™¨ v6.0 (Model Fetcher Ready)")
    print("  - /submit_injection_job, /get_injection_job (ç”¨äºåˆå§‹æ³¨å…¥)")
    print("  - /submit_prompt, /get_prompt_job (ç”¨äºå‘èµ·å¯¹è¯)")
    print("  - /submit_tool_result, /get_tool_result_job (ç”¨äºè¿”å›å·¥å…·ç»“æœ)")
    print("  - /submit_model_fetch_job, /get_model_fetch_job (ç”¨äºè·å–æ¨¡å‹)")
    print("  - /stream_chunk, /get_chunk (ç”¨äºæµå¼ä¼ è¾“)")
    print("  å·²åœ¨ http://127.0.0.1:5101 å¯åŠ¨")
    print("======================================================================")
    app.run(host='0.0.0.0', port=5101, threaded=True)