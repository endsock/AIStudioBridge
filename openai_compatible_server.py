# openai_compatible_server.py (v5.1 - Robust Tool Handling)

import requests
import json
import time
import sys
import re
import uuid
from flask import Flask, request, Response, jsonify
from flask_cors import CORS
from datetime import datetime, timedelta

# --- é…ç½® ---
PUBLIC_PORT = 5100
INTERNAL_SERVER_URL = "http://127.0.0.1:5101"
END_OF_STREAM_SIGNAL = "__END_OF_STREAM__"
MODEL_CACHE_TTL_SECONDS = 3600 # æ¨¡å‹åˆ—è¡¨ç¼“å­˜1å°æ—¶

# ã€æ–°ã€‘ä¸ºæœ¬åœ°è¿æ¥å®šä¹‰æ— ä»£ç†è®¾ç½®ï¼Œé¿å…ç³»ç»Ÿä»£ç†å¹²æ‰°
LOCAL_REQUEST_PROXIES = {
    "http": None,
    "https": None
}

app = Flask(__name__)
CORS(app)

LAST_CONVERSATION_STATE = None
MODEL_LIST_CACHE = {
    "data": None,
    "timestamp": 0
}


# --- OpenAI æ ¼å¼åŒ–è¾…åŠ©å‡½æ•° (å‡çº§) ---

# ã€æµå¼ã€‘æ–‡æœ¬å—
def format_openai_chunk(content: str, model: str, request_id: str):
    chunk_data = {"id": request_id, "object": "chat.completion.chunk", "created": int(time.time()), "model": model, "choices": [{"index": 0, "delta": {"content": content}, "finish_reason": None}]}
    return f"data: {json.dumps(chunk_data)}\n\n"

# ã€æµå¼ã€‘å·¥å…·è°ƒç”¨å— (å‡çº§ä»¥æ”¯æŒå¹¶è¡Œ)
def format_openai_tool_call_chunks(tool_calls: list, model: str, request_id: str):
    chunks = []
    for i, tool_call in enumerate(tool_calls):
        chunk_data = {
            "id": request_id,
            "object": "chat.completion.chunk",
            "created": int(time.time()),
            "model": model,
            "choices": [{
                "index": 0,
                "delta": {
                    "content": None,
                    "tool_calls": [{
                        "index": i, # <-- å…³é”®ï¼šæ¯ä¸ªè°ƒç”¨æœ‰è‡ªå·±çš„ç´¢å¼•
                        "id": tool_call['id'],
                        "type": "function",
                        "function": { "name": tool_call['function']['name'], "arguments": "" }
                    }]
                },
                "finish_reason": None
            }]
        }
        # å‘é€å‡½æ•°å
        chunks.append(f"data: {json.dumps(chunk_data)}\n\n")

        # å‘é€å‚æ•°
        chunk_data["choices"][0]["delta"]["tool_calls"][0]["function"]["arguments"] = tool_call['function']['arguments']
        chunks.append(f"data: {json.dumps(chunk_data)}\n\n")

    return "".join(chunks)

# ã€æµå¼ã€‘ç»“æŸå—
def format_openai_finish_chunk(model: str, request_id: str, finish_reason: str = "stop"):
    chunk_data = {"id": request_id, "object": "chat.completion.chunk", "created": int(time.time()), "model": model, "choices": [{"index": 0, "delta": {}, "finish_reason": finish_reason}]}
    return f"data: {json.dumps(chunk_data)}\n\n"

# ã€éæµå¼ã€‘å“åº”æ ¼å¼åŒ–å‡½æ•° (å‡çº§ä»¥æ”¯æŒå¹¶è¡Œ)
def format_openai_non_stream_response(content: str, tool_calls: list, model: str, request_id: str, finish_reason: str):
    message = {"role": "assistant"}
    if tool_calls:
        message["tool_calls"] = tool_calls
    else:
        message["content"] = content

    response_data = {
        "id": request_id,
        "object": "chat.completion",
        "created": int(time.time()),
        "model": model,
        "choices": [{"index": 0, "message": message, "finish_reason": finish_reason}],
        "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}
    }
    # ã€ã€ã€æ–°æ—¥å¿—ã€‘ã€‘ã€‘
    print("\n--- ğŸ“¦ [Non-Stream] æœ€ç»ˆå“åº”ä½“ ---")
    print(json.dumps(response_data, indent=2, ensure_ascii=False))
    print("----------------------------------\n")
    return response_data

# --- Google å“åº”è§£æä¸ä»»åŠ¡å¤„ç† (æ ¸å¿ƒå‡çº§) ---

# v5 è§£æå™¨ (ä¿æŒä¸å˜)
def _extract_value(value_wrapper):
    current_payload = value_wrapper
    while isinstance(current_payload, list):
        non_null_items = [item for item in current_payload if item is not None]
        if len(non_null_items) == 1: current_payload = non_null_items[0]
        else: break
    if not isinstance(current_payload, list): return current_payload
    if not current_payload: return []
    first_item = current_payload[0]
    if isinstance(first_item, list) and len(first_item) == 2 and isinstance(first_item[0], str):
        return convert_google_args_to_dict(current_payload)
    else:
        return [_extract_value(item) for item in current_payload]

def convert_google_args_to_dict(args_list: list) -> dict:
    if not isinstance(args_list, list): return {}
    params = {}
    for item in args_list:
        if isinstance(item, list) and len(item) == 2 and isinstance(item[0], str):
            key, value_wrapper = item[0], item[1]
            params[key] = _extract_value(value_wrapper)
    return params

# ã€ã€ã€æ ¸å¿ƒå‡çº§ï¼šè§£ææ‰€æœ‰å‡½æ•°è°ƒç”¨ã€‘ã€‘ã€‘
def parse_final_buffer_for_tool_calls(buffer: str):
    """
    åœ¨æµç»“æŸåï¼Œè§£ææ•´ä¸ªç¼“å†²åŒºä»¥æå–ã€æ‰€æœ‰ã€‘å‡½æ•°è°ƒç”¨ã€‚
    è¿”å›ä¸€ä¸ªå‡½æ•°è°ƒç”¨å¯¹è±¡çš„åˆ—è¡¨ï¼Œå¦‚æœæ‰¾ä¸åˆ°åˆ™è¿”å›ç©ºåˆ—è¡¨ã€‚
    """
    all_tool_calls = []
    try:
        # ã€ã€ã€æ ¸å¿ƒä¿®å¤ v2ï¼šæ›´ç¨³å¥åœ°å¤„ç†æ‹¼æ¥çš„JSONã€‘ã€‘ã€‘
        clean_buffer = buffer.strip()
        if not clean_buffer:
            all_chunks = []
        else:
            # Googleçš„æµå¼å“åº”å¯èƒ½æ˜¯å¤šä¸ªJSONæ•°ç»„é€šè¿‡æ¢è¡Œç¬¦æˆ–ç›´æ¥æ‹¼æ¥è€Œæˆ (e.g., "[...]\n[...]...][...")
            # æˆ‘ä»¬éœ€è¦åœ¨å®ƒä»¬ä¹‹é—´æ’å…¥é€—å·ï¼Œä½¿å…¶æˆä¸ºä¸€ä¸ªæœ‰æ•ˆçš„JSONæ•°ç»„ã€‚
            # 1. ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å¤„ç†åœ¨ "]" å’Œ "[" ä¹‹é—´å¯èƒ½å­˜åœ¨çš„ç©ºç™½å’Œæ¢è¡Œç¬¦
            processed_buffer = re.sub(r'\]\s*\[', '],[', clean_buffer)
            
            # 2. å°†æ•´ä¸ªç»“æœåŒ…è£¹åœ¨æ–¹æ‹¬å·ä¸­ï¼Œå½¢æˆä¸€ä¸ªå•ä¸€çš„ã€æœ‰æ•ˆçš„JSONæ•°ç»„å­—ç¬¦ä¸²
            full_json_str = f"[{processed_buffer}]"
            
            all_chunks = json.loads(full_json_str)
        
        # é€’å½’æŸ¥æ‰¾æ‰€æœ‰å‡½æ•°è°ƒç”¨ç»“æ„ä½“: `["function_name", [[args]]]`
        def find_all_calls_recursive(data):
            found_calls = []
            # æ£€æŸ¥å½“å‰èŠ‚ç‚¹æ˜¯å¦æ˜¯å‡½æ•°è°ƒç”¨
            if (isinstance(data, list) and len(data) > 0 and isinstance(data[0], str) and data[0] and
                    len(data) > 1 and isinstance(data[1], list) and len(data[1]) > 0 and isinstance(data[1][0], list)):
                return [data] # æ‰¾åˆ°äº†ä¸€ä¸ªï¼Œè¿”å›ä¸€ä¸ªåŒ…å«å®ƒçš„åˆ—è¡¨
            
            # å¦‚æœä¸æ˜¯ï¼Œé€’å½’æœç´¢å­èŠ‚ç‚¹
            if isinstance(data, list):
                for item in data:
                    found_calls.extend(find_all_calls_recursive(item))
            return found_calls

        for chunk in reversed(all_chunks):
            if not isinstance(chunk, list): continue
            if "Model generated function call(s)." in str(chunk):
                # ä»è¿™ä¸ªæ ‡å¿—æ€§å—å¼€å§‹é€’å½’æœç´¢
                raw_calls = find_all_calls_recursive(chunk)
                for call_data in raw_calls:
                    function_name = call_data[0]
                    arguments_dict = convert_google_args_to_dict(call_data[1][0])
                    all_tool_calls.append({
                        "id": f"call_{uuid.uuid4()}",
                        "type": "function",
                        "function": {
                            "name": function_name,
                            "arguments": json.dumps(arguments_dict, ensure_ascii=False)
                        }
                    })
                # æ‰¾åˆ°æ ‡å¿—å—åå°±å¤„ç†å¹¶é€€å‡ºï¼Œé¿å…é‡å¤è§£æ
                if all_tool_calls:
                    break
    except Exception as e:
        print(f"ğŸš¨ [Tool Call Parser Error] è§£ææœ€ç»ˆç¼“å†²åŒºæ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {type(e).__name__}: {e}")
    
    return all_tool_calls

def _internal_task_processor(task_id: str):
    # (æ­¤å‡½æ•°æ— éœ€æ›´æ”¹)
    start_time = time.time()
    while time.time() - start_time < 120:
        try:
            res = requests.get(f"{INTERNAL_SERVER_URL}/get_chunk/{task_id}", timeout=5, proxies=LOCAL_REQUEST_PROXIES)
            if res.status_code == 200:
                data = res.json()
                if data['status'] == 'ok': yield data.get('chunk')
                elif data['status'] == 'done':
                    yield END_OF_STREAM_SIGNAL
                    return
            time.sleep(0.05)
        except requests.exceptions.RequestException: time.sleep(1)
    yield END_OF_STREAM_SIGNAL

def _update_conversation_state(request_base, new_messages: list):
    """
    é€šç”¨çŠ¶æ€æ›´æ–°å‡½æ•°ã€‚
    - request_base: ä¸åŒ…å«æ–°æ¶ˆæ¯çš„åŸºç¡€è¯·æ±‚ã€‚
    - new_messages: ä¸€ä¸ªåŒ…å« 'user'/'tool' å’Œ 'assistant' æ¶ˆæ¯çš„åˆ—è¡¨ã€‚
    """
    global LAST_CONVERSATION_STATE
    new_state = request_base.copy()
    new_state["messages"].extend(new_messages)
    LAST_CONVERSATION_STATE = new_state
    print(f"âœ… [Cache] ä¼šè¯çŠ¶æ€å·²æ›´æ–°ï¼Œæ–°å¢ {len(new_messages)} æ¡æ¶ˆæ¯ã€‚")

# --- ä¸»å¤„ç†é€»è¾‘ (å‡çº§ä»¥æ”¯æŒå¹¶è¡Œ) ---

def stream_and_update_state(task_id: str, request_base: dict, user_or_tool_message: dict):
    model = request_base.get("model", "gemini-custom")
    request_id = f"chatcmpl-{uuid.uuid4()}"
    text_pattern = re.compile(r'\[\s*null\s*,\s*\"((?:\\.|[^\"\\])*)\"')
    full_raw_response_buffer = ""
    full_ai_response_text = ""

    print("... ğŸŸ¢ [Stream Mode] å¼€å§‹å®æ—¶ä¼ è¾“ ...")
    for chunk_content in _internal_task_processor(task_id):
        if chunk_content == END_OF_STREAM_SIGNAL: break
        full_raw_response_buffer += chunk_content
        matches = text_pattern.findall(chunk_content)
        for match_group in matches:
            try:
                # findallç›´æ¥è¿”å›æ•è·ç»„çš„å†…å®¹
                text = json.loads(f'"{match_group}"')
                if text and not text.startswith("**"):
                    full_ai_response_text += text
                    yield format_openai_chunk(text, model, request_id)
            except json.JSONDecodeError:
                continue

    print("... ğŸŸ¡ [Stream Mode] æµç»“æŸï¼Œè§£ææœ€ç»ˆç»“æœ ...")
    final_tool_calls = parse_final_buffer_for_tool_calls(full_raw_response_buffer)
    finish_reason = "stop"
    assistant_message = {"role": "assistant"}

    if final_tool_calls:
        print(f"âœ… [Stream Mode] æˆåŠŸè§£æ {len(final_tool_calls)} ä¸ªå·¥å…·è°ƒç”¨ã€‚")
        finish_reason = "tool_calls"
        assistant_message["tool_calls"] = final_tool_calls
        yield format_openai_tool_call_chunks(final_tool_calls, model, request_id)
    else:
        assistant_message["content"] = full_ai_response_text
    
    _update_conversation_state(request_base, [user_or_tool_message, assistant_message])
    yield format_openai_finish_chunk(model, request_id, finish_reason)
    yield "data: [DONE]\n\n"

def generate_non_streaming_response(task_id: str, request_base: dict, user_or_tool_message: dict):
    model = request_base.get("model", "gemini-custom")
    request_id = f"chatcmpl-{uuid.uuid4()}"
    text_pattern = re.compile(r'\[\s*null\s*,\s*\"((?:\\.|[^\"\\])*)\"')
    full_raw_response_buffer = ""
    full_ai_response_text = ""

    print("... ğŸŸ¢ [Non-Stream Mode] åœ¨åå°æ”¶é›†æ‰€æœ‰æ•°æ® ...")
    for chunk_content in _internal_task_processor(task_id):
        if chunk_content == END_OF_STREAM_SIGNAL: break
        full_raw_response_buffer += chunk_content
        matches = text_pattern.findall(chunk_content)
        for match_group in matches:
            try:
                # findallç›´æ¥è¿”å›æ•è·ç»„çš„å†…å®¹
                text = json.loads(f'"{match_group}"')
                if text and not text.startswith("**"):
                    full_ai_response_text += text
            except json.JSONDecodeError:
                continue
    
    print("... ğŸŸ¡ [Non-Stream Mode] æ”¶é›†å®Œæˆï¼Œè§£ææœ€ç»ˆç»“æœ ...")
    final_tool_calls = parse_final_buffer_for_tool_calls(full_raw_response_buffer)
    finish_reason = "stop"
    assistant_message = {"role": "assistant"}

    if final_tool_calls:
        print(f"âœ… [Non-Stream Mode] æˆåŠŸè§£æ {len(final_tool_calls)} ä¸ªå·¥å…·è°ƒç”¨ã€‚")
        finish_reason = "tool_calls"
        assistant_message["tool_calls"] = final_tool_calls
    else:
        assistant_message["content"] = full_ai_response_text
    
    _update_conversation_state(request_base, [user_or_tool_message, assistant_message])
    
    final_json_response = format_openai_non_stream_response(
        full_ai_response_text,
        final_tool_calls,
        model,
        request_id,
        finish_reason
    )
    return final_json_response

# --- æœåŠ¡å™¨è·¯ç”±ä¸ä¸»é€»è¾‘ (ä¿æŒä¸å˜) ---
def check_internal_server():
    print("...æ­£åœ¨æ£€æŸ¥å†…éƒ¨æœåŠ¡å™¨çŠ¶æ€...")
    try:
        response = requests.get(INTERNAL_SERVER_URL, timeout=3, proxies=LOCAL_REQUEST_PROXIES)
        if response.status_code == 200:
            print(f"âœ… å†…éƒ¨æœåŠ¡å™¨ (åœ¨ {INTERNAL_SERVER_URL}) è¿æ¥æˆåŠŸï¼")
            return True
    except requests.exceptions.RequestException:
        print("\n" + "!"*60); print("!! è‡´å‘½é”™è¯¯ï¼šæ— æ³•è¿æ¥åˆ°å†…éƒ¨æœåŠ¡å™¨ï¼"); print(f"!! è¯·ç¡®ä¿ `local_history_server.py` å·²ç»å¯åŠ¨å¹¶ä¸”æ­£åœ¨ {INTERNAL_SERVER_URL} ä¸Šè¿è¡Œã€‚"); print("!"*60); return False

def _normalize_message_content(message: dict) -> dict:
    content = message.get("content");
    if isinstance(content, list):
        message["content"] = "\n\n".join([p.get("text", "") for p in content if isinstance(p, dict) and p.get("type") == "text"])
    return message

def _inject_history(job_payload: dict, wait_time: int = 15):
    try:
        requests.post(f"{INTERNAL_SERVER_URL}/submit_injection_job", json=job_payload, proxies=LOCAL_REQUEST_PROXIES).raise_for_status()
        time.sleep(wait_time); return True
    except requests.exceptions.RequestException: return False

def _submit_prompt(prompt: str):
    try:
        response = requests.post(f"{INTERNAL_SERVER_URL}/submit_prompt", json={"prompt": prompt}, proxies=LOCAL_REQUEST_PROXIES)
        response.raise_for_status(); return response.json()['task_id']
    except requests.exceptions.RequestException: return None

def _submit_tool_result(result: str):
    """
    ä¸ºå·¥å…·å‡½æ•°è¿”å›ç»“æœåˆ›å»ºä¸€ä¸ªæ–°çš„ä»»åŠ¡ï¼Œå¹¶å°†å…¶æäº¤åˆ°å†…éƒ¨æœåŠ¡å™¨ã€‚
    è¿”å›ä¸€ä¸ªæ–°çš„ task_id ç”¨äºè·Ÿè¸ª AI çš„åç»­å“åº”ã€‚
    """
    try:
        new_task_id = str(uuid.uuid4())
        payload = {"task_id": new_task_id, "result": result}
        response = requests.post(f"{INTERNAL_SERVER_URL}/submit_tool_result", json=payload, proxies=LOCAL_REQUEST_PROXIES)
        response.raise_for_status()
        print(f"âœ… [API Gateway] å·²ä¸ºå·¥å…·è¿”å›ç»“æœåˆ›å»ºå¹¶æäº¤æ–°ä»»åŠ¡ (ID: {new_task_id[:8]})ã€‚")
        return new_task_id
    except requests.exceptions.RequestException as e:
        print(f"ğŸš¨ [API Gateway] æäº¤å·¥å…·ç»“æœå¤±è´¥: {e}")
        return None


@app.route('/reset_state', methods=['POST'])
def reset_state():
    global LAST_CONVERSATION_STATE; LAST_CONVERSATION_STATE = None
    print("ğŸ”„ [Cache] ä¼šè¯ç¼“å­˜å·²è¢«æ‰‹åŠ¨é‡ç½®ã€‚")
    return jsonify({"status": "success", "message": "Conversation cache has been reset."})

@app.route('/v1/chat/completions', methods=['POST', 'OPTIONS'])
def chat_completions():
    if request.method == 'OPTIONS': return '', 200
    global LAST_CONVERSATION_STATE
    print(f"\n[{time.strftime('%Y-%m-%d %H:%M:%S')}] æ¥æ”¶åˆ°æ–°çš„ /v1/chat/completions è¯·æ±‚...")
    request_data = request.json
    try:
        messages = [_normalize_message_content(msg) for msg in request_data.get("messages", [])]
        request_data["messages"] = messages
    except Exception as e: return jsonify({"error": f"å¤„ç†æ¶ˆæ¯å†…å®¹æ—¶å¤±è´¥: {e}"}), 400
    if not messages: return jsonify({"error": "'messages' åˆ—è¡¨ä¸èƒ½ä¸ºç©ºã€‚"}), 400

    use_stream = request_data.get('stream', False)
    print(f"æ¨¡å¼æ£€æµ‹: stream={use_stream}")
    is_continuation = False
    if LAST_CONVERSATION_STATE:
        cached_messages, new_messages_base = LAST_CONVERSATION_STATE.get("messages", []), messages[:-1]
        # æ£€æŸ¥æ˜¯å¦æ˜¯è¿ç»­å¯¹è¯ï¼ˆç”¨æˆ·æˆ–å·¥å…·ï¼‰
        if json.dumps(cached_messages, sort_keys=True) == json.dumps(new_messages_base, sort_keys=True):
            last_message_role = messages[-1].get("role")
            if last_message_role in ["user", "tool"]:
                 is_continuation = True

    task_id, last_message, request_base_for_update = None, None, None
    
    if is_continuation:
        last_message = messages[-1]
        request_base_for_update = request_data.copy()
        request_base_for_update["messages"] = messages[:-1] # æ›´æ–°çŠ¶æ€æ—¶åªç”¨åŸºç¡€éƒ¨åˆ†

        if last_message.get("role") == "user":
            print("âš¡ï¸ [Fast Path] æ£€æµ‹åˆ°è¿ç»­ã€ç”¨æˆ·å¯¹è¯ã€‘ï¼Œè·³è¿‡é¡µé¢åˆ·æ–°ã€‚")
            task_id = _submit_prompt(last_message.get("content"))
            if not task_id:
                LAST_CONVERSATION_STATE = None
                return jsonify({"error": "å¿«é€Ÿé€šé“æäº¤Promptå¤±è´¥"}), 500
        
        elif last_message.get("role") == "tool":
            print("ï¸ï¸ï¸âš¡ï¸ [Fast Path] æ£€æµ‹åˆ°ã€å·¥å…·ç»“æœè¿”å›ã€‘ï¼Œå‡†å¤‡æäº¤ã€‚")
            tool_result_content = last_message.get("content", "")
            task_id = _submit_tool_result(tool_result_content)
            if not task_id:
                LAST_CONVERSATION_STATE = None
                return jsonify({"error": "æäº¤å·¥å…·ç»“æœå¤±è´¥"}), 500

    else: # æ–°å¯¹è¯æˆ–çŠ¶æ€ä¸ä¸€è‡´
        print("ğŸ”„ [Full Injection] æ£€æµ‹åˆ°æ–°å¯¹è¯æˆ–çŠ¶æ€ä¸ä¸€è‡´ï¼Œæ‰§è¡Œå®Œæ•´é¡µé¢æ³¨å…¥ã€‚")
        LAST_CONVERSATION_STATE = None
        injection_payload = request_data.copy()
        last_message = messages[-1] if messages else None

        if last_message and last_message.get("role") == "user":
            injection_payload["messages"] = messages[:-1]
        else:
            last_message = None

        request_base_for_update = injection_payload
        
        if not _inject_history(injection_payload):
            return jsonify({"error": "æ³¨å…¥å†å²è®°å½•å¤±è´¥ã€‚"}), 500
        
        if last_message:
            task_id = _submit_prompt(last_message.get("content"))
        else:
            _update_conversation_state(request_base_for_update, [])
            model = request_data.get("model", "gemini-custom")
            req_id = f"chatcmpl-{uuid.uuid4()}"
            if use_stream:
                return Response(f"{format_openai_finish_chunk(model, req_id, 'stop')}data: [DONE]\n\n", mimetype='text/event-stream')
            else:
                return jsonify(format_openai_non_stream_response("", [], model, req_id, "stop"))

    if not task_id:
        return jsonify({"error": "æœªèƒ½è·å–ä»»åŠ¡ID"}), 500

    if use_stream:
        return Response(stream_and_update_state(task_id, request_base_for_update, last_message), mimetype='text/event-stream')
    else:
        return jsonify(generate_non_streaming_response(task_id, request_base_for_update, last_message))

# --- ã€ã€ã€æ–°ã€‘ã€‘ã€‘æ¨¡å‹åˆ—è¡¨ API ---

def parse_google_models_to_openai_format(google_models_json: str) -> list:
    """è§£ææ¥è‡ª Google AI Studio çš„åŸå§‹æ¨¡å‹æ•°æ®å¹¶å°†å…¶è½¬æ¢ä¸º OpenAI æ ¼å¼ã€‚"""
    try:
        # ç§»é™¤å¯èƒ½å­˜åœ¨çš„å‰ç¼€ï¼Œç¡®ä¿æ˜¯æœ‰æ•ˆçš„ JSON
        clean_json_str = google_models_json.strip()
        if not clean_json_str.startswith('['):
            clean_json_str = clean_json_str[clean_json_str.find('['):]
        
        data = json.loads(clean_json_str)
        model_list = []
        
        # å“åº”ä½“æ˜¯ä¸€ä¸ªåµŒå¥—å±‚çº§å¾ˆæ·±çš„æ•°ç»„
        all_model_data = data[0]

        for model_data in all_model_data:
            try:
                internal_id = model_data[0]
                # æœ‰äº›æ¨¡å‹IDå¯èƒ½ä¸å«'/'ï¼Œåšå¥½å…¼å®¹
                model_id = internal_id.split('/')[-1] if '/' in internal_id else internal_id
                display_name = model_data[3]
                
                # åˆ›å»ºä¸€ä¸ªç¬¦åˆ OpenAI æ ¼å¼çš„å­—å…¸
                model_entry = {
                    "id": model_id,
                    "object": "model",
                    "created": int(time.time()), # ä½¿ç”¨å½“å‰æ—¶é—´ä½œä¸ºåˆ›å»ºæ—¶é—´
                    "owned_by": "google",
                    # æ·»åŠ é¢å¤–å…ƒæ•°æ®ä»¥ä¾¿å®¢æˆ·ç«¯ä½¿ç”¨
                    "internal_id": internal_id,
                    "display_name": display_name,
                    "description": model_data[4] if len(model_data) > 4 else "No description available.",
                    "max_context_tokens": model_data[8] if len(model_data) > 8 else 0,
                    "max_output_tokens": model_data[9] if len(model_data) > 9 else 0,
                    "top_p": model_data[12] if len(model_data) > 12 else 0.0,
                    "top_k": model_data[13] if len(model_data) > 13 else 0
                }
                model_list.append(model_entry)
            except (IndexError, TypeError) as e:
                print(f"âš ï¸ [Model Parser] è§£ææ¨¡å‹æ¡ç›®æ—¶è·³è¿‡ä¸€ä¸ªæ ¼å¼ä¸ç¬¦çš„æ¡ç›®: {e} - æ¡ç›®: {str(model_data)[:100]}")
                continue
        
        print(f"âœ… [Model Parser] æˆåŠŸè§£æå¹¶è½¬æ¢äº† {len(model_list)} ä¸ªæ¨¡å‹ã€‚")
        return model_list
    except (json.JSONDecodeError, IndexError, TypeError) as e:
        print(f"ğŸš¨ [Model Parser] è§£ææ•´ä¸ªæ¨¡å‹åˆ—è¡¨æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
        return []

def fetch_and_cache_models():
    """è·å–å¹¶ç¼“å­˜æ¨¡å‹åˆ—è¡¨ã€‚å¦‚æœç¼“å­˜æœ‰æ•ˆåˆ™ç›´æ¥è¿”å›ï¼Œå¦åˆ™è§¦å‘æ–°çš„è·å–æµç¨‹ã€‚"""
    global MODEL_LIST_CACHE
    
    # æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
    cache_age = time.time() - MODEL_LIST_CACHE['timestamp']
    if MODEL_LIST_CACHE['data'] and cache_age < MODEL_CACHE_TTL_SECONDS:
        print("âœ… [Model Cache] æ¨¡å‹åˆ—è¡¨ç¼“å­˜æœ‰æ•ˆï¼Œç›´æ¥è¿”å›ã€‚")
        return MODEL_LIST_CACHE['data']

    print("ğŸ”„ [Model Fetcher] æ¨¡å‹åˆ—è¡¨ç¼“å­˜ä¸å­˜åœ¨æˆ–å·²è¿‡æœŸï¼Œå¼€å§‹æ–°çš„è·å–æµç¨‹...")
    try:
        # 1. è§¦å‘æ²¹çŒ´è„šæœ¬å¼€å§‹è·å–
        print("...[Model Fetcher] 1/3 - å‘é€è·å–ä»»åŠ¡åˆ°æœ¬åœ°æœåŠ¡å™¨...")
        res_submit = requests.post(f"{INTERNAL_SERVER_URL}/submit_model_fetch_job", timeout=5, proxies=LOCAL_REQUEST_PROXIES)
        res_submit.raise_for_status()

        # 2. ç­‰å¾…æ²¹çŒ´è„šæœ¬è¿”å›æ•°æ®
        print("...[Model Fetcher] 2/3 - ç­‰å¾…æ²¹çŒ´è„šæœ¬è¿”å›æ¨¡å‹æ•°æ® (æœ€é•¿60ç§’)...")
        res_get = requests.get(f"{INTERNAL_SERVER_URL}/get_reported_models", timeout=65, proxies=LOCAL_REQUEST_PROXIES)
        res_get.raise_for_status()
        
        response_data = res_get.json()
        if response_data.get('status') != 'success':
            raise Exception(f"è·å–æ¨¡å‹æ•°æ®å¤±è´¥: {response_data.get('message', 'æœªçŸ¥é”™è¯¯')}")

        raw_models_json = response_data.get('data')

        # 3. è§£æå¹¶ç¼“å­˜ç»“æœ
        print("...[Model Fetcher] 3/3 - è§£æå¹¶ç¼“å­˜æ–°çš„æ¨¡å‹åˆ—è¡¨...")
        formatted_models = parse_google_models_to_openai_format(raw_models_json)
        
        MODEL_LIST_CACHE['data'] = formatted_models
        MODEL_LIST_CACHE['timestamp'] = time.time()

        return formatted_models

    except requests.exceptions.RequestException as e:
        print(f"ğŸš¨ [Model Fetcher] ä¸æœ¬åœ°æœåŠ¡å™¨é€šä¿¡å¤±è´¥: {e}")
        return None
    except Exception as e:
        print(f"ğŸš¨ [Model Fetcher] è·å–æ¨¡å‹åˆ—è¡¨è¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        return None

@app.route('/v1/models', methods=['GET'])
def list_models():
    """å®ç° OpenAI çš„ /v1/models æ¥å£ã€‚"""
    print(f"\n[{time.strftime('%Y-%m-%d %H:%M:%S')}] æ¥æ”¶åˆ°æ–°çš„ /v1/models è¯·æ±‚...")
    
    models = fetch_and_cache_models()
    
    if models is None:
        return jsonify({"error": "æ— æ³•ä»å†…éƒ¨æœåŠ¡å™¨è·å–æ¨¡å‹åˆ—è¡¨ã€‚"}), 500
        
    response_data = {
      "object": "list",
      "data": models
    }
    
    return jsonify(response_data)


if __name__ == "__main__":
    if not check_internal_server(): sys.exit(1)
    print("="*60); print("  OpenAI å…¼å®¹ API ç½‘å…³ v6.0 (Model Fetcher Ready)"); print("="*60)
    print("  âœ¨ æ–°åŠŸèƒ½: æ”¯æŒé€šè¿‡ /v1/models åŠ¨æ€è·å–æ¨¡å‹åˆ—è¡¨ã€‚")
    print("  âœ¨ æ–°åŠŸèƒ½: æ”¯æŒé€šè¿‡ 'role: tool' æ¶ˆæ¯è¿”å›å‡½æ•°æ‰§è¡Œç»“æœã€‚")
    print("\n  è¿è¡ŒæŒ‡å—:"); print("  1. âœ… `local_history_server.py` å·²æˆåŠŸè¿æ¥ã€‚"); print("  2. âœ… ç¡®ä¿æµè§ˆå™¨å’Œæ²¹çŒ´è„šæœ¬å·²å°±ç»ªã€‚"); print(f"  3. ğŸš€ æœ¬ API æœåŠ¡å™¨æ­£åœ¨ http://127.0.0.1:{PUBLIC_PORT} ä¸Šè¿è¡Œã€‚"); print("="*60)
    app.run(host='0.0.0.0', port=PUBLIC_PORT, threaded=True)