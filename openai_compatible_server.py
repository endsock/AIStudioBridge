# openai_compatible_server.py (v5.1 - Robust Tool Handling)

import requests
import json
import time
import sys
import re
import uuid
from flask import Flask, request, Response, jsonify
from flask_cors import CORS

# --- é…ç½® ---
PUBLIC_PORT = 5100
INTERNAL_SERVER_URL = "http://127.0.0.1:5101"
END_OF_STREAM_SIGNAL = "__END_OF_STREAM__"

app = Flask(__name__)
CORS(app)

LAST_CONVERSATION_STATE = None

# --- OpenAI æ ¼å¼åŒ–è¾…åŠ©å‡½æ•° (å‡çº§) ---

# ã€æµå¼ã€‘æ–‡æœ¬å—
def format_openai_chunk(content: str, model: str, request_id: str):
    chunk_data = {"id": request_id, "object": "chat.completion.chunk", "created": int(time.time()), "model": model, "choices": [{"index": 0, "delta": {"content": content}, "finish_reason": None}]}
    return f"data: {json.dumps(chunk_data)}\n\n"

# ã€æµå¼ã€‘å·¥å…·è°ƒç”¨å— (å‡çº§ä»¥æ”¯æŒå¹¶è¡Œ)
def format_openai_tool_call_chunks(tool_calls: list, model: str, request_id: str):
    chunks = []
    for i, tool_call in enumerate(tool_calls):
        chunk_data = {
            "id": request_id,
            "object": "chat.completion.chunk",
            "created": int(time.time()),
            "model": model,
            "choices": [{
                "index": 0,
                "delta": {
                    "content": None,
                    "tool_calls": [{
                        "index": i, # <-- å…³é”®ï¼šæ¯ä¸ªè°ƒç”¨æœ‰è‡ªå·±çš„ç´¢å¼•
                        "id": tool_call['id'],
                        "type": "function",
                        "function": { "name": tool_call['function']['name'], "arguments": "" }
                    }]
                },
                "finish_reason": None
            }]
        }
        # å‘é€å‡½æ•°å
        chunks.append(f"data: {json.dumps(chunk_data)}\n\n")

        # å‘é€å‚æ•°
        chunk_data["choices"][0]["delta"]["tool_calls"][0]["function"]["arguments"] = tool_call['function']['arguments']
        chunks.append(f"data: {json.dumps(chunk_data)}\n\n")

    return "".join(chunks)

# ã€æµå¼ã€‘ç»“æŸå—
def format_openai_finish_chunk(model: str, request_id: str, finish_reason: str = "stop"):
    chunk_data = {"id": request_id, "object": "chat.completion.chunk", "created": int(time.time()), "model": model, "choices": [{"index": 0, "delta": {}, "finish_reason": finish_reason}]}
    return f"data: {json.dumps(chunk_data)}\n\n"

# ã€éæµå¼ã€‘å“åº”æ ¼å¼åŒ–å‡½æ•° (å‡çº§ä»¥æ”¯æŒå¹¶è¡Œ)
def format_openai_non_stream_response(content: str, tool_calls: list, model: str, request_id: str, finish_reason: str):
    message = {"role": "assistant"}
    if tool_calls:
        message["tool_calls"] = tool_calls
    else:
        message["content"] = content

    response_data = {
        "id": request_id,
        "object": "chat.completion",
        "created": int(time.time()),
        "model": model,
        "choices": [{"index": 0, "message": message, "finish_reason": finish_reason}],
        "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}
    }
    # ã€ã€ã€æ–°æ—¥å¿—ã€‘ã€‘ã€‘
    print("\n--- ğŸ“¦ [Non-Stream] æœ€ç»ˆå“åº”ä½“ ---")
    print(json.dumps(response_data, indent=2, ensure_ascii=False))
    print("----------------------------------\n")
    return response_data

# --- Google å“åº”è§£æä¸ä»»åŠ¡å¤„ç† (æ ¸å¿ƒå‡çº§) ---

# v5 è§£æå™¨ (ä¿æŒä¸å˜)
def _extract_value(value_wrapper):
    current_payload = value_wrapper
    while isinstance(current_payload, list):
        non_null_items = [item for item in current_payload if item is not None]
        if len(non_null_items) == 1: current_payload = non_null_items[0]
        else: break
    if not isinstance(current_payload, list): return current_payload
    if not current_payload: return []
    first_item = current_payload[0]
    if isinstance(first_item, list) and len(first_item) == 2 and isinstance(first_item[0], str):
        return convert_google_args_to_dict(current_payload)
    else:
        return [_extract_value(item) for item in current_payload]

def convert_google_args_to_dict(args_list: list) -> dict:
    if not isinstance(args_list, list): return {}
    params = {}
    for item in args_list:
        if isinstance(item, list) and len(item) == 2 and isinstance(item[0], str):
            key, value_wrapper = item[0], item[1]
            params[key] = _extract_value(value_wrapper)
    return params

# ã€ã€ã€æ ¸å¿ƒå‡çº§ï¼šè§£ææ‰€æœ‰å‡½æ•°è°ƒç”¨ã€‘ã€‘ã€‘
def parse_final_buffer_for_tool_calls(buffer: str):
    """
    åœ¨æµç»“æŸåï¼Œè§£ææ•´ä¸ªç¼“å†²åŒºä»¥æå–ã€æ‰€æœ‰ã€‘å‡½æ•°è°ƒç”¨ã€‚
    è¿”å›ä¸€ä¸ªå‡½æ•°è°ƒç”¨å¯¹è±¡çš„åˆ—è¡¨ï¼Œå¦‚æœæ‰¾ä¸åˆ°åˆ™è¿”å›ç©ºåˆ—è¡¨ã€‚
    """
    all_tool_calls = []
    try:
        clean_buffer = buffer.strip().lstrip(',')
        full_json_str = f"[{clean_buffer}]"
        all_chunks = json.loads(full_json_str)
        
        # é€’å½’æŸ¥æ‰¾æ‰€æœ‰å‡½æ•°è°ƒç”¨ç»“æ„ä½“: `["function_name", [[args]]]`
        def find_all_calls_recursive(data):
            found_calls = []
            # æ£€æŸ¥å½“å‰èŠ‚ç‚¹æ˜¯å¦æ˜¯å‡½æ•°è°ƒç”¨
            if (isinstance(data, list) and len(data) > 0 and isinstance(data[0], str) and data[0] and
                    len(data) > 1 and isinstance(data[1], list) and len(data[1]) > 0 and isinstance(data[1][0], list)):
                return [data] # æ‰¾åˆ°äº†ä¸€ä¸ªï¼Œè¿”å›ä¸€ä¸ªåŒ…å«å®ƒçš„åˆ—è¡¨
            
            # å¦‚æœä¸æ˜¯ï¼Œé€’å½’æœç´¢å­èŠ‚ç‚¹
            if isinstance(data, list):
                for item in data:
                    found_calls.extend(find_all_calls_recursive(item))
            return found_calls

        for chunk in reversed(all_chunks):
            if not isinstance(chunk, list): continue
            if "Model generated function call(s)." in str(chunk):
                # ä»è¿™ä¸ªæ ‡å¿—æ€§å—å¼€å§‹é€’å½’æœç´¢
                raw_calls = find_all_calls_recursive(chunk)
                for call_data in raw_calls:
                    function_name = call_data[0]
                    arguments_dict = convert_google_args_to_dict(call_data[1][0])
                    all_tool_calls.append({
                        "id": f"call_{uuid.uuid4()}",
                        "type": "function",
                        "function": {
                            "name": function_name,
                            "arguments": json.dumps(arguments_dict, ensure_ascii=False)
                        }
                    })
                # æ‰¾åˆ°æ ‡å¿—å—åå°±å¤„ç†å¹¶é€€å‡ºï¼Œé¿å…é‡å¤è§£æ
                if all_tool_calls:
                    break
    except Exception as e:
        print(f"ğŸš¨ [Tool Call Parser Error] è§£ææœ€ç»ˆç¼“å†²åŒºæ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {type(e).__name__}: {e}")
    
    return all_tool_calls

def _internal_task_processor(task_id: str):
    # (æ­¤å‡½æ•°æ— éœ€æ›´æ”¹)
    start_time = time.time()
    while time.time() - start_time < 120:
        try:
            res = requests.get(f"{INTERNAL_SERVER_URL}/get_chunk/{task_id}", timeout=5)
            if res.status_code == 200:
                data = res.json()
                if data['status'] == 'ok': yield data.get('chunk')
                elif data['status'] == 'done':
                    yield END_OF_STREAM_SIGNAL
                    return
            time.sleep(0.05)
        except requests.exceptions.RequestException: time.sleep(1)
    yield END_OF_STREAM_SIGNAL

def _update_conversation_state(request_base, new_messages: list):
    """
    é€šç”¨çŠ¶æ€æ›´æ–°å‡½æ•°ã€‚
    - request_base: ä¸åŒ…å«æ–°æ¶ˆæ¯çš„åŸºç¡€è¯·æ±‚ã€‚
    - new_messages: ä¸€ä¸ªåŒ…å« 'user'/'tool' å’Œ 'assistant' æ¶ˆæ¯çš„åˆ—è¡¨ã€‚
    """
    global LAST_CONVERSATION_STATE
    new_state = request_base.copy()
    new_state["messages"].extend(new_messages)
    LAST_CONVERSATION_STATE = new_state
    print(f"âœ… [Cache] ä¼šè¯çŠ¶æ€å·²æ›´æ–°ï¼Œæ–°å¢ {len(new_messages)} æ¡æ¶ˆæ¯ã€‚")

# --- ä¸»å¤„ç†é€»è¾‘ (å‡çº§ä»¥æ”¯æŒå¹¶è¡Œ) ---

def stream_and_update_state(task_id: str, request_base: dict, user_or_tool_message: dict):
    model = request_base.get("model", "gemini-custom")
    request_id = f"chatcmpl-{uuid.uuid4()}"
    text_pattern = re.compile(r'\[\s*null\s*,\s*\"((?:\\.|[^\"\\])*)\"')
    full_raw_response_buffer = ""
    full_ai_response_text = ""

    print("... ğŸŸ¢ [Stream Mode] å¼€å§‹å®æ—¶ä¼ è¾“ ...")
    for chunk_content in _internal_task_processor(task_id):
        if chunk_content == END_OF_STREAM_SIGNAL: break
        full_raw_response_buffer += chunk_content
        match = text_pattern.search(chunk_content)
        if match:
            try:
                text = json.loads(f'"{match.group(1)}"')
                if text and not text.startswith("**"):
                    full_ai_response_text += text
                    yield format_openai_chunk(text, model, request_id)
            except json.JSONDecodeError: continue

    print("... ğŸŸ¡ [Stream Mode] æµç»“æŸï¼Œè§£ææœ€ç»ˆç»“æœ ...")
    final_tool_calls = parse_final_buffer_for_tool_calls(full_raw_response_buffer)
    finish_reason = "stop"
    assistant_message = {"role": "assistant"}

    if final_tool_calls:
        print(f"âœ… [Stream Mode] æˆåŠŸè§£æ {len(final_tool_calls)} ä¸ªå·¥å…·è°ƒç”¨ã€‚")
        finish_reason = "tool_calls"
        assistant_message["tool_calls"] = final_tool_calls
        yield format_openai_tool_call_chunks(final_tool_calls, model, request_id)
    else:
        assistant_message["content"] = full_ai_response_text
    
    _update_conversation_state(request_base, [user_or_tool_message, assistant_message])
    yield format_openai_finish_chunk(model, request_id, finish_reason)
    yield "data: [DONE]\n\n"

def generate_non_streaming_response(task_id: str, request_base: dict, user_or_tool_message: dict):
    model = request_base.get("model", "gemini-custom")
    request_id = f"chatcmpl-{uuid.uuid4()}"
    text_pattern = re.compile(r'\[\s*null\s*,\s*\"((?:\\.|[^\"\\])*)\"')
    full_raw_response_buffer = ""
    full_ai_response_text = ""

    print("... ğŸŸ¢ [Non-Stream Mode] åœ¨åå°æ”¶é›†æ‰€æœ‰æ•°æ® ...")
    for chunk_content in _internal_task_processor(task_id):
        if chunk_content == END_OF_STREAM_SIGNAL: break
        full_raw_response_buffer += chunk_content
        match = text_pattern.search(chunk_content)
        if match:
            try:
                text = json.loads(f'"{match.group(1)}"')
                if text and not text.startswith("**"):
                    full_ai_response_text += text
            except json.JSONDecodeError: continue
    
    print("... ğŸŸ¡ [Non-Stream Mode] æ”¶é›†å®Œæˆï¼Œè§£ææœ€ç»ˆç»“æœ ...")
    final_tool_calls = parse_final_buffer_for_tool_calls(full_raw_response_buffer)
    finish_reason = "stop"
    assistant_message = {"role": "assistant"}

    if final_tool_calls:
        print(f"âœ… [Non-Stream Mode] æˆåŠŸè§£æ {len(final_tool_calls)} ä¸ªå·¥å…·è°ƒç”¨ã€‚")
        finish_reason = "tool_calls"
        assistant_message["tool_calls"] = final_tool_calls
    else:
        assistant_message["content"] = full_ai_response_text
    
    _update_conversation_state(request_base, [user_or_tool_message, assistant_message])
    
    final_json_response = format_openai_non_stream_response(
        full_ai_response_text,
        final_tool_calls,
        model,
        request_id,
        finish_reason
    )
    return final_json_response

# --- æœåŠ¡å™¨è·¯ç”±ä¸ä¸»é€»è¾‘ (ä¿æŒä¸å˜) ---
def check_internal_server():
    print("...æ­£åœ¨æ£€æŸ¥å†…éƒ¨æœåŠ¡å™¨çŠ¶æ€...")
    try:
        response = requests.get(INTERNAL_SERVER_URL, timeout=3)
        if response.status_code == 200:
            print(f"âœ… å†…éƒ¨æœåŠ¡å™¨ (åœ¨ {INTERNAL_SERVER_URL}) è¿æ¥æˆåŠŸï¼")
            return True
    except requests.exceptions.RequestException:
        print("\n" + "!"*60); print("!! è‡´å‘½é”™è¯¯ï¼šæ— æ³•è¿æ¥åˆ°å†…éƒ¨æœåŠ¡å™¨ï¼"); print(f"!! è¯·ç¡®ä¿ `local_history_server.py` å·²ç»å¯åŠ¨å¹¶ä¸”æ­£åœ¨ {INTERNAL_SERVER_URL} ä¸Šè¿è¡Œã€‚"); print("!"*60); return False

def _normalize_message_content(message: dict) -> dict:
    content = message.get("content");
    if isinstance(content, list):
        message["content"] = "\n\n".join([p.get("text", "") for p in content if isinstance(p, dict) and p.get("type") == "text"])
    return message

def _inject_history(job_payload: dict, wait_time: int = 15):
    try:
        requests.post(f"{INTERNAL_SERVER_URL}/submit_injection_job", json=job_payload).raise_for_status()
        time.sleep(wait_time); return True
    except requests.exceptions.RequestException: return False

def _submit_prompt(prompt: str):
    try:
        response = requests.post(f"{INTERNAL_SERVER_URL}/submit_prompt", json={"prompt": prompt})
        response.raise_for_status(); return response.json()['task_id']
    except requests.exceptions.RequestException: return None

def _submit_tool_result(result: str):
    """
    ä¸ºå·¥å…·å‡½æ•°è¿”å›ç»“æœåˆ›å»ºä¸€ä¸ªæ–°çš„ä»»åŠ¡ï¼Œå¹¶å°†å…¶æäº¤åˆ°å†…éƒ¨æœåŠ¡å™¨ã€‚
    è¿”å›ä¸€ä¸ªæ–°çš„ task_id ç”¨äºè·Ÿè¸ª AI çš„åç»­å“åº”ã€‚
    """
    try:
        new_task_id = str(uuid.uuid4())
        payload = {"task_id": new_task_id, "result": result}
        response = requests.post(f"{INTERNAL_SERVER_URL}/submit_tool_result", json=payload)
        response.raise_for_status()
        print(f"âœ… [API Gateway] å·²ä¸ºå·¥å…·è¿”å›ç»“æœåˆ›å»ºå¹¶æäº¤æ–°ä»»åŠ¡ (ID: {new_task_id[:8]})ã€‚")
        return new_task_id
    except requests.exceptions.RequestException as e:
        print(f"ğŸš¨ [API Gateway] æäº¤å·¥å…·ç»“æœå¤±è´¥: {e}")
        return None


@app.route('/reset_state', methods=['POST'])
def reset_state():
    global LAST_CONVERSATION_STATE; LAST_CONVERSATION_STATE = None
    print("ğŸ”„ [Cache] ä¼šè¯ç¼“å­˜å·²è¢«æ‰‹åŠ¨é‡ç½®ã€‚")
    return jsonify({"status": "success", "message": "Conversation cache has been reset."})

@app.route('/v1/chat/completions', methods=['POST', 'OPTIONS'])
def chat_completions():
    if request.method == 'OPTIONS': return '', 200
    global LAST_CONVERSATION_STATE
    print(f"\n[{time.strftime('%Y-%m-%d %H:%M:%S')}] æ¥æ”¶åˆ°æ–°çš„ /v1/chat/completions è¯·æ±‚...")
    request_data = request.json
    try:
        messages = [_normalize_message_content(msg) for msg in request_data.get("messages", [])]
        request_data["messages"] = messages
    except Exception as e: return jsonify({"error": f"å¤„ç†æ¶ˆæ¯å†…å®¹æ—¶å¤±è´¥: {e}"}), 400
    if not messages: return jsonify({"error": "'messages' åˆ—è¡¨ä¸èƒ½ä¸ºç©ºã€‚"}), 400

    use_stream = request_data.get('stream', False)
    print(f"æ¨¡å¼æ£€æµ‹: stream={use_stream}")
    is_continuation = False
    if LAST_CONVERSATION_STATE:
        cached_messages, new_messages_base = LAST_CONVERSATION_STATE.get("messages", []), messages[:-1]
        # æ£€æŸ¥æ˜¯å¦æ˜¯è¿ç»­å¯¹è¯ï¼ˆç”¨æˆ·æˆ–å·¥å…·ï¼‰
        if json.dumps(cached_messages, sort_keys=True) == json.dumps(new_messages_base, sort_keys=True):
            last_message_role = messages[-1].get("role")
            if last_message_role in ["user", "tool"]:
                 is_continuation = True

    task_id, last_message, request_base_for_update = None, None, None
    
    if is_continuation:
        last_message = messages[-1]
        request_base_for_update = request_data.copy()
        request_base_for_update["messages"] = messages[:-1] # æ›´æ–°çŠ¶æ€æ—¶åªç”¨åŸºç¡€éƒ¨åˆ†

        if last_message.get("role") == "user":
            print("âš¡ï¸ [Fast Path] æ£€æµ‹åˆ°è¿ç»­ã€ç”¨æˆ·å¯¹è¯ã€‘ï¼Œè·³è¿‡é¡µé¢åˆ·æ–°ã€‚")
            task_id = _submit_prompt(last_message.get("content"))
            if not task_id:
                LAST_CONVERSATION_STATE = None
                return jsonify({"error": "å¿«é€Ÿé€šé“æäº¤Promptå¤±è´¥"}), 500
        
        elif last_message.get("role") == "tool":
            print("ï¸ï¸ï¸âš¡ï¸ [Fast Path] æ£€æµ‹åˆ°ã€å·¥å…·ç»“æœè¿”å›ã€‘ï¼Œå‡†å¤‡æäº¤ã€‚")
            tool_result_content = last_message.get("content", "")
            task_id = _submit_tool_result(tool_result_content)
            if not task_id:
                LAST_CONVERSATION_STATE = None
                return jsonify({"error": "æäº¤å·¥å…·ç»“æœå¤±è´¥"}), 500

    else: # æ–°å¯¹è¯æˆ–çŠ¶æ€ä¸ä¸€è‡´
        print("ğŸ”„ [Full Injection] æ£€æµ‹åˆ°æ–°å¯¹è¯æˆ–çŠ¶æ€ä¸ä¸€è‡´ï¼Œæ‰§è¡Œå®Œæ•´é¡µé¢æ³¨å…¥ã€‚")
        LAST_CONVERSATION_STATE = None
        injection_payload = request_data.copy()
        last_message = messages[-1] if messages else None

        if last_message and last_message.get("role") == "user":
            injection_payload["messages"] = messages[:-1]
        else:
            last_message = None

        request_base_for_update = injection_payload
        
        if not _inject_history(injection_payload):
            return jsonify({"error": "æ³¨å…¥å†å²è®°å½•å¤±è´¥ã€‚"}), 500
        
        if last_message:
            task_id = _submit_prompt(last_message.get("content"))
        else:
            _update_conversation_state(request_base_for_update, [])
            model = request_data.get("model", "gemini-custom")
            req_id = f"chatcmpl-{uuid.uuid4()}"
            if use_stream:
                return Response(f"{format_openai_finish_chunk(model, req_id, 'stop')}data: [DONE]\n\n", mimetype='text/event-stream')
            else:
                return jsonify(format_openai_non_stream_response("", [], model, req_id, "stop"))

    if not task_id:
        return jsonify({"error": "æœªèƒ½è·å–ä»»åŠ¡ID"}), 500

    if use_stream:
        return Response(stream_and_update_state(task_id, request_base_for_update, last_message), mimetype='text/event-stream')
    else:
        return jsonify(generate_non_streaming_response(task_id, request_base_for_update, last_message))

if __name__ == "__main__":
    if not check_internal_server(): sys.exit(1)
    print("="*60); print("  OpenAI å…¼å®¹ API ç½‘å…³ v5.1 (Robust Tool Handling)"); print("="*60)
    print("  âœ¨ æ–°åŠŸèƒ½: æ”¯æŒé€šè¿‡ 'role: tool' æ¶ˆæ¯è¿”å›å‡½æ•°æ‰§è¡Œç»“æœã€‚")
    print("  âœ¨ ä¿®å¤: ç¡®ä¿ä¸ºå·¥å…·è¿”å›çš„å“åº”æµæ­£ç¡®åˆå§‹åŒ–ä»»åŠ¡ã€‚")
    print("\n  è¿è¡ŒæŒ‡å—:"); print("  1. âœ… `local_history_server.py` å·²æˆåŠŸè¿æ¥ã€‚"); print("  2. âœ… ç¡®ä¿æµè§ˆå™¨å’Œæ²¹çŒ´è„šæœ¬å·²å°±ç»ªã€‚"); print(f"  3. ğŸš€ æœ¬ API æœåŠ¡å™¨æ­£åœ¨ http://127.0.0.1:{PUBLIC_PORT} ä¸Šè¿è¡Œã€‚"); print("="*60)
    app.run(host='0.0.0.0', port=PUBLIC_PORT, threaded=True)