# openai_compatible_server.py

import requests
import json
import time
import sys
import re
import uuid
from flask import Flask, request, Response, jsonify
from flask_cors import CORS # ã€ã€ã€æ–°ã€‘ã€‘ã€‘ å¼•å…¥ CORS

# --- é…ç½® ---
PUBLIC_PORT = 5100 
INTERNAL_SERVER_URL = "http://127.0.0.1:5101"
END_OF_STREAM_SIGNAL = "__END_OF_STREAM__"

app = Flask(__name__)
# --- ã€ã€ã€æ ¸å¿ƒä¿®å¤ï¼šä¸ºæ•´ä¸ªåº”ç”¨å¯ç”¨ CORSã€‘ã€‘ã€‘ ---
# è¿™å°†è‡ªåŠ¨å¤„ç†æ‰€æœ‰ OPTIONS é¢„æ£€è¯·æ±‚ï¼Œå¹¶æ·»åŠ å¿…è¦çš„ Access-Control-* å¤´ä¿¡æ¯ã€‚
CORS(app)
# --- ã€ã€ã€ä¿®å¤ç»“æŸã€‘ã€‘ã€‘ ---

LAST_CONVERSATION_STATE = None

def check_internal_server():
    print("...æ­£åœ¨æ£€æŸ¥å†…éƒ¨æœåŠ¡å™¨çŠ¶æ€...")
    try:
        response = requests.get(INTERNAL_SERVER_URL, timeout=3)
        if response.status_code == 200:
            print(f"âœ… å†…éƒ¨æœåŠ¡å™¨ (åœ¨ {INTERNAL_SERVER_URL}) è¿žæŽ¥æˆåŠŸï¼")
            return True
    except requests.exceptions.RequestException:
        print("\n" + "!"*60)
        print("!! è‡´å‘½é”™è¯¯ï¼šæ— æ³•è¿žæŽ¥åˆ°å†…éƒ¨æœåŠ¡å™¨ï¼")
        print(f"!! è¯·ç¡®ä¿ `local_history_server.py` å·²ç»å¯åŠ¨å¹¶ä¸”æ­£åœ¨ {INTERNAL_SERVER_URL} ä¸Šè¿è¡Œã€‚")
        print("!"*60)
        return False

def _normalize_message_content(message: dict) -> dict:
    content = message.get("content")
    if isinstance(content, list):
        all_text_parts = []
        for part in content:
            if isinstance(part, dict) and part.get("type") == "text":
                all_text_parts.append(part.get("text", ""))
        message["content"] = "\n\n".join(all_text_parts)
    return message

def _inject_history(job_payload: dict, wait_time: int = 10):
    try:
        requests.post(f"{INTERNAL_SERVER_URL}/submit_injection_job", json=job_payload).raise_for_status()
        time.sleep(wait_time)
        return True
    except requests.exceptions.RequestException as e: return False

def _submit_prompt(prompt: str):
    try:
        response = requests.post(f"{INTERNAL_SERVER_URL}/submit_prompt", json={"prompt": prompt})
        response.raise_for_status()
        return response.json()['task_id']
    except requests.exceptions.RequestException as e: return None

def format_openai_chunk(content: str, model: str, request_id: str):
    chunk_data = {"id": request_id, "object": "chat.completion.chunk", "created": int(time.time()), "model": model, "choices": [{"index": 0, "delta": {"content": content}, "finish_reason": None}]}
    return f"data: {json.dumps(chunk_data)}\n\n"

def format_openai_finish_chunk(model: str, request_id: str):
    chunk_data = {"id": request_id, "object": "chat.completion.chunk", "created": int(time.time()), "model": model, "choices": [{"index": 0, "delta": {}, "finish_reason": "stop"}]}
    return f"data: {json.dumps(chunk_data)}\n\n"

def stream_and_update_state(task_id: str, request_base: dict, final_prompt: str):
    global LAST_CONVERSATION_STATE
    model = request_base.get("model", "gemini-custom")
    request_id = f"chatcmpl-{uuid.uuid4()}"
    text_pattern = re.compile(r'\[\s*null\s*,\s*\"((?:\\.|[^\"\\])*)\"(?:\s*,\s*\"model\")?\s*\]')
    buffer = ""
    full_ai_response_text = ""
    stream_ended_properly = False
    start_time = time.time()
    while time.time() - start_time < 120:
        try:
            res = requests.get(f"{INTERNAL_SERVER_URL}/get_chunk/{task_id}", timeout=5)
            if res.status_code == 200:
                data = res.json()
                if data['status'] == 'ok':
                    chunk_content = data.get('chunk')
                    if chunk_content == END_OF_STREAM_SIGNAL:
                        stream_ended_properly = True; break
                    buffer += chunk_content
                    last_pos = 0
                    for match in text_pattern.finditer(buffer):
                        try:
                            text = json.loads(f'"{match.group(1)}"')
                            full_ai_response_text += text
                            yield format_openai_chunk(text, model, request_id)
                        except json.JSONDecodeError: continue
                        last_pos = match.end()
                    buffer = buffer[last_pos:]
                elif data['status'] == 'done':
                    stream_ended_properly = True; break
            time.sleep(0.05)
        except requests.exceptions.RequestException: time.sleep(1)
    if stream_ended_properly:
        new_state = request_base.copy()
        new_state["messages"].append({"role": "user", "content": final_prompt})
        new_state["messages"].append({"role": "assistant", "content": full_ai_response_text})
        LAST_CONVERSATION_STATE = new_state
        print("âœ… [Cache] ä¼šè¯çŠ¶æ€å·²æ­£ç¡®æ›´æ–°ã€‚")
    else:
        LAST_CONVERSATION_STATE = None
        print("âš ï¸ [Cache] æµæœªæ­£å¸¸ç»“æŸï¼Œä¼šè¯ç¼“å­˜å·²æ¸…ç©ºã€‚")
    yield format_openai_finish_chunk(model, request_id)
    yield "data: [DONE]\n\n"

@app.route('/reset_state', methods=['POST'])
def reset_state():
    global LAST_CONVERSATION_STATE
    LAST_CONVERSATION_STATE = None
    print("ðŸ”„ [Cache] ä¼šè¯ç¼“å­˜å·²è¢«æ‰‹åŠ¨é‡ç½®ã€‚")
    return jsonify({"status": "success", "message": "Conversation cache has been reset."})

@app.route('/v1/chat/completions', methods=['POST', 'OPTIONS'])
def chat_completions():
    # OPTIONSè¯·æ±‚ç”±Flask-CORSè‡ªåŠ¨å¤„ç†ï¼Œæˆ‘ä»¬ä¸éœ€è¦æ˜¾å¼åœ°å¤„ç†å®ƒã€‚
    # å½“çœŸæ­£çš„POSTè¯·æ±‚åˆ°è¾¾æ—¶ï¼Œè¿™ä¸ªå‡½æ•°æ‰ä¼šæ‰§è¡Œã€‚
    if request.method == 'OPTIONS':
        return '', 200

    global LAST_CONVERSATION_STATE
    print(f"\n[{time.strftime('%Y-%m-%d %H:%M:%S')}] æŽ¥æ”¶åˆ°æ–°çš„ /v1/chat/completions è¯·æ±‚...")
    request_data = request.json
    try:
        messages = [_normalize_message_content(msg) for msg in request_data.get("messages", [])]
        request_data["messages"] = messages
    except Exception as e: return f"é”™è¯¯ï¼šå¤„ç†æ¶ˆæ¯å†…å®¹æ—¶å¤±è´¥: {e}", 400
    if not request_data.get('stream', False): return "é”™è¯¯ï¼šæ­¤æœåŠ¡å™¨ä»…æ”¯æŒæµå¼å“åº” (stream: true)ã€‚", 400
    if not messages: return "é”™è¯¯: 'messages' åˆ—è¡¨ä¸èƒ½ä¸ºç©ºã€‚", 400

    is_continuation = False
    if LAST_CONVERSATION_STATE:
        cached_messages = LAST_CONVERSATION_STATE.get("messages", [])
        new_messages_base = messages[:-1]
        cached_dump = json.dumps(cached_messages, sort_keys=True)
        new_base_dump = json.dumps(new_messages_base, sort_keys=True)
        if cached_dump == new_base_dump and messages[-1].get("role") == "user":
            is_continuation = True
    
    if is_continuation:
        print("âš¡ï¸ [Fast Path] æ£€æµ‹åˆ°è¿žç»­å¯¹è¯ï¼Œè·³è¿‡é¡µé¢åˆ·æ–°ã€‚")
        final_prompt = messages[-1].get("content")
        request_base_for_update = request_data.copy()
        request_base_for_update["messages"] = messages[:-1]
        task_id = _submit_prompt(final_prompt)
        if task_id: return Response(stream_and_update_state(task_id, request_base_for_update, final_prompt), mimetype='text/event-stream')
        else:
            LAST_CONVERSATION_STATE = None
            return "é”™è¯¯ï¼šå¿«é€Ÿé€šé“æäº¤Promptå¤±è´¥ï¼Œè¯·é‡è¯•ï¼ˆå°†æ‰§è¡Œå®Œæ•´æ³¨å…¥ï¼‰ã€‚", 500
    else:
        print("ðŸ”„ [Full Injection] æ£€æµ‹åˆ°æ–°å¯¹è¯æˆ–çŠ¶æ€ä¸ä¸€è‡´ï¼Œæ‰§è¡Œå®Œæ•´é¡µé¢æ³¨å…¥ã€‚")
        LAST_CONVERSATION_STATE = None
        injection_payload = request_data.copy()
        final_prompt = None
        if messages[-1].get("role") == "user":
            injection_payload["messages"] = messages[:-1]
            final_prompt = messages[-1].get("content")
        if _inject_history(injection_payload):
            if final_prompt:
                task_id = _submit_prompt(final_prompt)
                if task_id: return Response(stream_and_update_state(task_id, injection_payload, final_prompt), mimetype='text/event-stream')
            else:
                LAST_CONVERSATION_STATE = injection_payload
                print("âœ… [Cache] ä»…æ³¨å…¥ä»»åŠ¡å®Œæˆï¼Œä¼šè¯çŠ¶æ€å·²æ›´æ–°ã€‚")
                def empty_stream():
                    yield format_openai_finish_chunk(request_data.get("model", "gemini-custom"), f"chatcmpl-{uuid.uuid4()}")
                    yield "data: [DONE]\n\n"
                return Response(empty_stream(), mimetype='text/event-stream')
    return "é”™è¯¯ï¼šæœªèƒ½å¤„ç†è¯·æ±‚ï¼Œå‡ºçŽ°æœªçŸ¥é”™è¯¯ã€‚", 500

if __name__ == "__main__":
    if not check_internal_server(): sys.exit(1)
    print("="*60)
    print("  OpenAI å…¼å®¹ API ç½‘å…³ v2.5 (The CORS Guardian)")
    print("="*60)
    print("  âœ¨ æ–°åŠŸèƒ½: å·²å¯ç”¨ CORS æ”¯æŒï¼Œå¯ä»¥å¤„ç†æ¥è‡ªä»»ä½•å‰ç«¯åº”ç”¨çš„è·¨åŸŸè¯·æ±‚ã€‚")
    print("\n  è¿è¡ŒæŒ‡å—:")
    print("  1. âœ… `local_history_server.py` å·²æˆåŠŸè¿žæŽ¥ã€‚")
    print("  2. âœ… ç¡®ä¿æµè§ˆå™¨å’Œæ²¹çŒ´è„šæœ¬å·²å°±ç»ªã€‚")
    print(f"  3. ðŸš€ æœ¬ API æœåŠ¡å™¨æ­£åœ¨ http://127.0.0.1:{PUBLIC_PORT} ä¸Šè¿è¡Œã€‚")
    print("="*60)
    app.run(host='0.0.0.0', port=PUBLIC_PORT, threaded=True)